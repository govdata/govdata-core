'''
Routines for obtaining, generating, and processing meta data about data files, 
file runs, and functions used in the data file generation process. 

The way that the metadata scheme works is:

In the top level System Folder there is a directory ../System/MetaData that 
contains a replica of the file structure outside of the System folder.	 
Each "real" path corresponds to a directory in the MetaData directory,
in which metadata about the path is stored, 
e.g. '../Data/Dan_Data/NPR_Puzzle_Solutions', would correspond to 
'../System/MetaData/Data/Dan_Data/NPR_Puzzle_Solutions'.   
Metadata is similarly attached to functions in python modules referenced via their dot-paths.	
	
Given a file path P, the associated path for the metadata is given by the 
function metadatapath(P).  Given a python function dot path, the associated 
path P for the metadata is given by the function opmetadatapath(P).	  

Inside a given path's metadata directory is a variety of associated metadata files. 
There are three main kinds of associated things:
	-- data generated during the runtime that produced/was generated by the path. 
	-- metadata attached by the human directly to the file.	 
	-- data used and generated by the system Graphical Browser.	 
	e.g. Linklists local to the path, and graphviz-generated graphs of these LinkLists.	 
	(See commends in System/SystemGraphOperations.py for information about this) 
	
'''


from System.Utils import *
import marshal, os, types, hashlib, traceback
import tabular as tb
import cPickle as pickle
import System.StaticAnalysis
from System.Storage import StoredDocstring


def AttachMetaData(NewMetaData,FileName = '',OperationName='',Resources = None, creates = ('../System/MetaData/',)):

	'''
	Attach metadata to a file a given path.	   
	
	ARGUMENTS:
	--NewMetaData : a dictionary that will be pickled as metadata.	
	--FileName = Path of the file to which the metadata should be attached.
	--OperationName = Python dot-path of the operati which the metadata should 
	be attached. 
	(Either File or OperationName should be specified, but not both.)
		
	Metadata attached through this process is put into the file 
		MetaDataPath/AttachedMetaData.pickle.
	If this path exists at the time that an AttachMetaData command is run,
	the metadata dictionary in the file is updated with the NewMetaData dictionary. 
		
	'''

	assert isinstance(NewMetaData,dict), 'The metadata to attach must be a dictionary.'
	
	if FileName != '':
		metapath = metadatapath(FileName)
	elif OperationName != '':
		metapath = opmetadatapath(OperationName)
	
	if not PathExists(metapath):
		MakeDirs(metapath)
	
	mdp = metapath + '/AttachedMetaData.pickle'
	if PathExists(mdp):
		try:
			ExistingMetaData = pickle.load(open(mdp,'rb'))
			if not isinstance(ExistingMetaData,dict):
				ExistingMetaData = {}
		except:
			ExistingMetaData = {}
	else:
		ExistingMetaData = {}
	
	ExistingMetaData.update(NewMetaData)
	
	F = open(mdp,'w')
	pickle.dump(ExistingMetaData,F)
	F.close()
	
	ProcessMetaData(metapath,FileName if FileName else OperationName,extensions=['Attached'])


def MakeRuntimeMetaData(opname,Creates,OriginalTimes,OriginalDirInfo,RunOutput,ExitType,ExitStatus,Before,After,IsDifferent,TempSOIS):

	'''
	This is an internal usage function that attaches the results of 
	metadata generated during the running of a system update.  
	It is used primilary by the UpdateLinks function in ../System/Update.py
	
	Suppose the function F is run by the automatic updater.	  
	During this runtime, several kinds of data are produced:  
	--the return output of the function F, and 
	--information about the run, its Exit status, whether it changed any 
		files it was meant to produce, etc...
	
	These pieces of runtime metadata are collected by the UpdateLink fnction 
	during update, and then passed to MakeRuntimeMetaData function to write 
	out the data in an appropriate format to the metadata files. 
	
	in summary, it
	-- writes output of the executed function F to a file at path 
		opmetadatapath(F) + '/RuntimeOutput.pickle'
	-- appends exit status information to opmetadatapath(F) + '/ExitStatusRecord.csv'
	-- for each file j created by the running of F:
		-- appends file-specific creation information to 
		metadatapath(j) + '/CreationRecord.csv'
		-- attaches MetaData returned by the script for file j to
		metadatapath(j) + '/AssociatedMetaData.pickle'

	'''
	
	MakeAutomaticMetaData(opname,usedefault=True)
	
	if not PathExists('../System/MetaData/'):
		MakeDir('../System/MetaData/')
	for j in Creates:
		if not PathExists(metadatapath(j)):
			MakeDirs(metadatapath(j))
	if not PathExists(opmetadatapath(opname)):
		MakeDirs(opmetadatapath(opname))
	
	rtime = str(After - Before)
	
	strongcopy(opmetadatapath(opname) + '/MostRecentRunPrintout.txt',opmetadatapath(opname) + '/PreviousRunPrintout.txt')
	delete(opmetadatapath(opname) + '/MostRecentRunPrintout.txt')
	strongcopy(TempSOIS,opmetadatapath(opname) + '/MostRecentRunPrintout.txt')

	for j in Creates:
		CRFName = metadatapath(j) + '/CreationRecord.csv'
		if not PathExists(CRFName):
			F = open(CRFName,'w')
			F.write('FileName,Operation,ExitType,TimeStamp,OriginalTimeStamp,Runtime,Diff\n')
			F.close()								
			
		F = open(CRFName,'a')
		F.write(','.join([j,opname,ExitType,str(os.path.getmtime(j) if PathExists(j) else nan),str(OriginalTimes[j]),rtime,str(int(IsDifferent[j]))]) + '\n')
		F.close()	
	
	for j in OriginalDirInfo.keys():
		CRFName = metadatapath(j) + '/CreationRecord.csv'
		Ptime = FindPtime(j)		
		if PathExists(CRFName):
			F = open(CRFName,'a')
			F.write(','.join([j,opname,'FromBelow',str(os.path.getmtime(j) if PathExists(j) else nan),str(OriginalDirInfo[j][0]),'0.0','1']) + '\n')
			F.close()	
	
	F = open(opmetadatapath(opname) + '/RuntimeOutput.pickle','w')
	pickle.dump(RunOutput,F)
	F.close()
	
	ESFName = opmetadatapath(opname) + '/ExitStatusFile.csv'
	if not PathExists(ESFName):
		F = open(ESFName,'w')
		F.write('OperationName,ExitType,ExitStatus,TimeStamp,CreateList,CreateTimeStamps,Before,After,Runtime\n')
		F.close()			
	CreateString = '\t'.join(Creates)
	CreateTimesList = [os.path.getmtime(l) if PathExists(l) else numpy.nan for l in Creates]
	CreateTimesString = '\t'.join([str(x) for x in CreateTimesList])
	TS = str(max(Before,max(CreateTimesList)))
	NewESFData = ','.join([opname,ExitType, str(ExitStatus),TS , CreateString , CreateTimesString,str(Before),str(After),rtime]) + '\n'
	esf = open(ESFName,'a') 
	esf.write(NewESFData)
	esf.close()		
				
	if ExitType == 'Success':
		Written = []
		if isinstance(RunOutput,dict) and 'MetaData' in RunOutput.keys() and isinstance(RunOutput['MetaData'],dict):	
			for j in RunOutput['MetaData'].keys():
				if isinstance(j,str):
					if any([PathAlong(j,k) for k in Creates]) or ('PROTECTION' not in os.environ.keys()) or os.environ['PROTECTION'] != 'ON':
						if not PathExists(metadatapath(j)):
							MakeDirs(metadatapath(j))
						mdp = metadatapath(j) + '/AssociatedMetaData.pickle'
						
						if PathExists(mdp):
							oldmetadata = pickle.load(open(mdp,'rb'))
						else:
							oldmetadata = None					
						newmetadata = RunOutput['MetaData'][j]
						if newmetadata != oldmetadata:
							metadatafile = open(mdp,'w')
							pickle.dump(newmetadata,metadatafile)
							metadatafile.close()
						print '\nMetaData for ', j, 'written ...'
						Written.append(mdp)			
						ProcessMetaData(metadatapath(j),objname=j,extensions=['Associated'])

					else:
						print 'Attempt to write runtime metadata for', j,  ' blocked because Protection is ON and ', j , ' is not contained in any of the declared creates: ', Creates, '.'
				else:
					print 'Attempt to write runtime metadata for', j, ' failed because', j ,'does not appear to be a path string.'
		else:
			print 'No runtime metadata output for ', opname , ', or metadata argument in wrong format.'
	
		BadSet = ListUnion([[kk for kk in RecursiveFileList(metadatapath(k)) if (kk.endswith('AssociatedMetaData.pickle')) and kk not in Written] for k in Creates])
		for kk in BadSet:
			'Deleting', kk, 'as it appears to old irrelevant associated metadata.'
			delete(kk)

	if ExitType == 'Success':
		for j in Creates:
			if IsDifferent[j] and PathExists(j):
				MakeAutomaticMetaData(j)

def MakeAutomaticMetaData(objname,usedefault=False,forced=False,creates = ('../System/MetaData',),**kwargs):
	print 'Generating automatic metadata for ', objname, '...'
	try:
		import System.config.SetupFunctions as SF
		reload(SF)
	except:
		print 'Failed to import SetupFunctions module.	Using default automatic metadata module.'
		X = DEFAULT_GenerateAutomaticMetaData(objname)
	else:
		if not usedefault and hasattr(SF,'GenerateAutomaticMetaData') and type(SF.GenerateAutomaticMetaData) == types.FunctionType:
			try:
				X = SF.GenerateAutomaticMetaData(objname,forced=forced,**kwargs)
			except:
				print 'Automatic generation of metadata for', objname, 'by user-defined function failed (using default instead).  Here is the error:'
				traceback.print_exc()
				X = DEFAULT_GenerateAutomaticMetaData(objname)
		else:
			X = DEFAULT_GenerateAutomaticMetaData(objname)
	
	metapath = opmetadatapath(objname) if IsDotPath(objname) else metadatapath(objname)
	
	if isinstance(X,dict):
		for j in X.keys():
			metapathj = opmetadatapath(j) if IsDotPath(j) else metadatapath(j)
			if PathAlong(metapathj,metapath) or (IsDotPath(j) and metapath == metapathj):
				if not PathExists(metapathj):
					MakeDirs(metapathj)
				F = open(metapathj + '/AutomaticMetaData.pickle','w')
				pickle.dump(X[j],F)
				F.close()
	
				ProcessMetaData(metapathj,j,extensions=['Automatic'])
		print '... done generating automatic metadata for ', objname, '.'
	else:
		print 'Automatically generated metadata for', objname, 'is not in proper format.'

	

def DEFAULT_GenerateAutomaticMetaData(objname):
	
	D = {}
	
	if IsPythonFile(objname) or IsDotPath(objname):
		D['Verbose'] = StoredDocstring(objname)

	if D:
		return	{objname : D}
	else:
		return {}


def ProcessMetaData(metapath,objname=None, extensions = None, usedefault=False,depends_on='../System/MetaData/'):
	if objname is None:
		objname = metapath

	try:
		import System.config.SetupFunctions as SF
	except :
		print 'Failed to import SetupFunction module.  Using Default MetaData Processor.'
		DEFAULT_MetaDataProcessor(metapath,objname=objname,extensions=extensions)
	else:
		if not usedefault and hasattr(SF,'MetaDataProcessor') and type(SF.MetaDataProcessor) == types.FunctionType:
			try:
				SF.MetaDataProcessor(metapath,objname=objname,extensions=extensions)
			except:
				print 'Processing of metadata for', objname, 'by user-defined Processor function failed	 (using default instead).  Here is the error:'
				traceback.print_exc()
				DEFAULT_MetaDataProcessor(metapath,objname=objname,extensions=extensions)
		else:
			DEFAULT_MetaDataProcessor(metapath, objname=objname, extensions=extensions)



def DEFAULT_MetaDataProcessor(metapath,objname = None, extensions = None):

	if objname is None:
		objname = metapath
	
	X = ConsolidateSources(metapath)
	ProcessResources(metapath,X,objname)
	image = ChooseImage(metapath)
	if image:
		X['image'] = image
	F = open(metapath + '/ProcessedMetaData.pickle','w')
	pickle.dump(X,F)
	
	text = SummarizeMetaData(X)
	F = open(metapath + '/MetaDataSummary.html','w')
	F.write(text)
	F.close()


def ConsolidateSources(metapath,objname=None,extensions = None):

	consolidated = CombineSources(metapath,keys = ['Resources','author','keywords','signature','title','description','Verbose'],extensions=extensions)
	
	if 'Resources' in consolidated:
		consolidated['Resources'] = uniqify(ListUnion(consolidated['Resources'].values()))
	
	if 'author' in consolidated.keys():
		consolidated['author'] = '; '.join(consolidated['author'].values())
	
	if 'title' in consolidated.keys():
		consolidated['title'] = '; '.join(consolidated['title'].values())
		
	if 'description' in consolidated.keys():
		descrs = consolidated['description'].items()
		if len(descrs) == 1:
			consolidated['description'] = descrs[0][1]
		else:
			consolidated['description'] = '\n\n'.join([e + ': ' + d for (e,d) in descrs])
			
	elif 'Verbose' in consolidated.keys():
		descrs = consolidated['Verbose'].items()
		if len(descrs) == 1:
			consolidated['description'] = descrs[0][1]
		else:
			consolidated['description'] = '\n\n'.join([e + ': ' + d for (e,d) in descrs])
	
	if 'keywords' in consolidated.keys():
		for k in consolidated['keywords'].keys():
			if not is_string_like(consolidated['keywords'][k]):
				consolidated['keywords'][k] = ','.join(consolidated['keywords'][k])
				
		consolidated['keywords'] = [x.strip() for x in uniqify((','.join(consolidated['keywords'].values())).split(','))]
				
	if 'signature' in consolidated.keys():
		s = uniqify(consolidated['signature'].values())
		if len(s) == 1:
			consolidated['signature'] = s[0]
		else:
			consolidated['signature'] = ''

	return consolidated
	
	
	
def SummarizeMetaData(X):

	if 'image' in X.keys():
		image = '<img src="' + X['image'] + '"/><br/>'
	else:
		image = ''

	if 'description' in X.keys():
		description = '<strong>Description: </strong>' + X['description'].replace('\n','<br/>')
	else:
		description = ''
	
	if 'author' in X.keys():
		author = '<strong>Author: </strong>' + X['author']
	else:
		author = ''
	
	if 'title' in X.keys():
		title = '<strong>Title: </strong>' + X['title']
	else:
		title = ''
	
	if 'keywords' in X.keys():
		keywords = '<strong>Keywords: </strong>' + ','.join(X['keywords'])
	else:
		keywords = ''
	
	if 'signature' in X.keys():
		signature = '<strong>Signature: </strong> This appears to be a ' + X['signature'] + ' file.'  
	else:
		signature = ''

	
	text = '<br/>'.join([x for x in [image,title,author,signature,description,keywords] if x != ''])
	
	return text
	
	
def ChooseImage(metapath):
	images = [metapath[2:] + '/' + l for l in listdir(metapath) if 'image' in l.lower() and l.lower().endswith(('.png','.pdf','.gif','.jpg','.tiff','.bmp'))]
	if len(images) > 0:
		return images[0]


def CombineSources(metapath,keys=None,extensions=None,depends_on='../System/MetaData/'):

	if extensions is None:
		extensions = ['Attached','Associated','Automatic']

	consolidated = {} 
	for ext in extensions:
		fileext = '/' + ext + 'MetaData.pickle'
		if PathExists(metapath + fileext):
			metadata = pickle.load(open(metapath + fileext,'r'))
			if is_string_like(metadata):
				metadata = {'description' : metadata}

			for k in metadata.keys():
				if keys is None or k in keys:
					if k not in consolidated:
						consolidated[k] = {ext : metadata[k] }
					else:
						consolidated[k].update({ext : metadata[k]})
	
	return consolidated


def ProcessResources(metapath,metadata,objname,depends_on='../'):
	if isinstance(metadata,dict) and 'Resources' in metadata.keys():
		Resources = metadata['Resources']
		if Resources:
			for (source,name) in Resources:
				if is_file_name(name):
					mdp = metapath + '/' + name
					if is_external_url(source):
						E = os.system('wget ' + source + ' -O ' + mdp)
						if E != 0 or not PathExists(mdp):
							print 'Error attaching metadata to', objname, ': appears to have been unable to locate download URL', source
					else:
						if is_string_like(source) and PathExists(source):
							try:
								strongcopy(source,mdp)
							except:
								print 'Error processing resource metadata to', objname, ': source name',source, 'does not describe a path on the system or a URL.'
						else:
							print 'Error processing resource metadata to', objname, ': source name',source, 'does not describe a path on the system or a URL.'
				else:
					print 'Error processing resource metadata to', objname, ': metadata name', name, 'isn\'t a file name'
				

def tabularmetadataforms(pathlist,depends_on = '../System/MetaData/'):
	attlist = ['description','author','title','keywords']
	recs1 = []
	recs2 = []
	for x in pathlist:
		print x
		mdp = metadatapath(x) + '/ProcessedMetaData.pickle'
		if PathExists(mdp):
			M = pickle.load(open(mdp))
			D = {}
			for att in attlist:
				if att in M.keys():
					D[att] = M[att]
				else:
					D[att] = ''
			recs1.append((x,) + tuple([D[att].replace('\n',' ') for att in attlist]))
			colnames = M['colnames']
			if 'coldescrs' in M.keys():
				coldescrs = [M['coldescrs'][m] if m in M['coldescrs'].keys() else ''  for m in colnames]
			else:
				coldescrs = ['']*len(colnames)
			
			recs2 += zip([x]*len(colnames),colnames,coldescrs)		
		
	X = tb.tabarray(records = recs1,names=['Path'] + attlist)
	Y = tb.tabarray(records = recs2,names = ['Path','ColName','ColDescr'])
	
	return [X,Y]
	
def copymetadata(path,to,depends_on='../System/MetaData/'):
	strongcopy(metadatapath(path) + '/ProcessedMetaData.pickle',to)
	
def loadmetadata(path,depends_on = '../System/MetaData/'):
	if PathExists(metadatapath(path) + '/ProcessedMetaData.pickle'):
		return pickle.load(open(metadatapath(path) + '/ProcessedMetaData.pickle','rb'))

def IsFailure(Path):
	'''
	Returns Boolean True if Path represents is the python dot-path 
	of an operation whose most recent run by the autmatic updater 
	was a failure.
	'''
	metapath = opmetadatapath(Path) + '/ExitStatusFile.csv'
	if PathExists(metapath):
		try:
			ESD = tb.tabarray(SVfile = metapath,delimiter = ',', lineterminator='\n') 
			ESD.sort(order = ['TimeStamp'])
			return ESD['ExitType'][-1] == 'Failure'
		except:
			return False
	else:
		return False
		
		
def GetBrokenOperations(depends_on=('../System/MetaData/',)):
	'''
		Returns list of operations that failed on their most recent run. 
	'''

	ES = depends_on[0]
	if PathExists(ES):
		BrokenNames = []
		#load exit status data
		CSVList = [x for x in RecursiveFileList(ES) if x.endswith('ExitStatusFile.csv')]
		if len(CSVList) > 0:
			for j in CSVList:
				ESD = tb.tabarray(SVfile = j,delimiter = ',', lineterminator='\n') 
				ESD.sort(order = ['TimeStamp'])
				if ESD['ExitType'][-1] == 'Failure':
					BrokenNames += [ESD['OperationName'][-1]]

			#return those operations whose most recent exit stats is not 0
			return BrokenNames
		else:
			return []
	else:
		print 'The Exit status file does not exist.'
		return []
	
	
	
def LastTimeChanged(path):
	'''
	Returns last time, according to runtime meta data, that a  file (at "path")
	was actually modified (e.g. not simply overwritten, but actually modified.)
	'''


	actualmodtime = os.path.getmtime(path)
	if actualmodtime == FindPtime(path):
		try: 
			Data = tb.tabarray(SVfile = metapath,delimiter = ',', lineterminator='\n') 
			if len(Data) > 0:
				Data.sort(order=['TimeStamp'])
				Diffs = Data['Diff'].nonzero()[0]
				if len(Diffs) > 0:
					return Data['TimeStamp'][Diffs[-1]]
				else:
					return actualmodtime
			else:
				return actualmodtime
		except:
			return actualmodtime
	else:
		return actualmodtime
									
									
									
def FindPtime(target,Simple=False):
	'''
	Returns last time, according to runtime meta data, that 
	a target was succesfully created, if it is created data. 
	'''

	metapath = metadatapath(target) + '/CreationRecord.csv'
	if PathExists(metapath):
		try: 
			Data = tb.tabarray(SVfile = metapath,delimiter = ',', lineterminator='\n') 
			if len(Data) > 0:
				Data.sort(order=['TimeStamp'])
				if any(Data['ExitType'] == 'Success'):
					MostRecentSuccess = Data[Data['ExitType'] == 'Success']['TimeStamp'][-1]
					MoreRecentFailures = Data[(Data['ExitType'] == 'Failure') & (Data['TimeStamp'] > MostRecentSuccess)]
					if len(MoreRecentFailures) > 0:
						LeastRecentFailure = MoreRecentFailures['TimeStamp'][0]
					else:
						LeastRecentFailure = numpy.inf
					return Data[(Data['TimeStamp'] >= MostRecentSuccess) & (Data['TimeStamp'] < LeastRecentFailure)]['TimeStamp'][-1] 
				else:
					return numpy.nan
			else:
				return numpy.nan
		except:
			return numpy.nan
		else: pass
	else:
		return numpy.nan			


def metadatapath(datapath):
	return '../System/MetaData/' + datapath.strip('../')
	
def opmetadatapath(oppath):
	return '../System/MetaData/' + oppath.replace('.','/')
